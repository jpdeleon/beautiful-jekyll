{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# everything about mcmc  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling methods\n",
    "See visualization of common methods [here](https://chi-feng.github.io/mcmc-demo/app.html). The description below is copied verbatim from [Ballnus+2017](https://bmcsystbiol.biomedcentral.com/articles/10.1186/s12918-017-0433-1).\n",
    "\n",
    "* Random Walk Metropolis-Hastings (MH)\n",
    " - The MH algorithm samples from the posterior via a weighted random walk. Parameter candidates are drawn from a proposal distribution and accepted or rejected based on the ratio of the posterior at the parameter candidate and the current parameter. The choice of the proposal distribution is a design parameter. In practice the distribution is frequently chosen to be symmetric, e.g., a normal distribution, and centered at the current point.\n",
    "\n",
    " - The MH algorithm has several shortcomings, including the need for manual tuning of the proposal covariance and high autocorrelation [39]. Accordingly, a large number of extensions have been developed. In the following, we introduce the three single-chain and the two multi-chain methods employed in this study. \n",
    " \n",
    "\n",
    "* Adaptive Metropolis (AM)\n",
    "  - The AM algorithm is an extension of the standard MH algorithm. Instead of using a fixed proposal distribution which is tuned manually, the distribution is updated based on the already available samples. In particular, for posteriors with high correlation, this improves sampling efficiency by aligning the proposal with the posterior distribution [31]. In addition to the correlation structure, the scale of the proposal is also adapted. A commonly applied scaling scheme is based on the dimension of the problem [28, 29] while other possible schemes are based on the chain acceptance rate [34]. These scaling schemes are in the following indicated by ‘dim’ and ‘acc’, respectively.\n",
    "\n",
    "\n",
    "* Delayed Rejection Adaptive Metropolis (DRAM)\n",
    " - To further decrease the in-chain auto-correlation, the AM algorithm has been combined with a delayed rejection method, yielding the DRAM algorithm [28]. When a candidate parameter is rejected, the algorithm tries to find a new point using the information about the rejected point. This is repeated multiple times until a certain number of tries is reached or a point is accepted. We employ the implementation provided in [28]. This implementation is exclusively based on the previously mentioned ‘dim’ adaption scheme.\n",
    "\n",
    "\n",
    "* Metropolis-adjusted Langevin Algorithm (MALA)\n",
    " - Both AM and DRAM work best if the local and the global shape of the posterior are similar. Otherwise, the performance of the algorithm suffers, i.e. the in-chain auto-correlation increases. To circumvent this problem, the MALA makes use of the gradient, ∇θp(θ|D), and Fisher Information Matrix [37] of the estimation problem at the current point in parameter space. This information is used to construct a proposal which is adapted to the local posterior shape [37, 42]. Gradient and Fisher Information Matrix can be computed using forward sensitivity equations [43].\n",
    "\n",
    "\n",
    "* Parallel Tempering (PT)\n",
    " - All of the algorithms, AM, DRAM and MALA, discussed so far are single-chain algorithms which exploit local posterior properties to tune their global movement. This can make transitions between different posterior modes unlikely if they are separated by areas of low probability density. To address the issue, PT algorithms have been introduced. These algorithm sample from multiple tempered versions of the posterior p(D|θ)1βlp(θ), β l ≥1, l=1,…,L, at the same time [33–35]. The tempered posteriors are flattened out in comparison to the posterior, rendering transitions between posterior modes more likely. Allowing the tempered chains to exchange their position by chance enables the untempered chain, which samples from the posterior, to ‘jump’. For this study, we have implemented the PT algorithm as formulated by Lacki et al. [32] using AM with ‘acc’ adaption scheme or MALA for each tempered chain.\n",
    " \n",
    " \n",
    "* Parallel Hierarchical Sampling (PHS)\n",
    " - An alternative to PT is PHS, which employs several chains sampling from the posterior [36]. Similar to PT, the idea is to start multiple auxiliary chains at different points in parameter space and to swap the main chain with a randomly picked one in each iteration. The main differences between PT and PHS are that all chains of PHS are sampling from the same distribution and that a swap between main and auxiliary chains is always accepted in PHS. The use of multiple chains can improve the mixing as different chains can employ different proposal distributions [5]. Here we apply AM(‘acc’) for each of the auxiliary chains.\n",
    " \n",
    "\n",
    "* Affine-invariant Ensemble\n",
    " - algorithm by [Goodman & Weare 2010]() and implementation by [Foreman-Mackey+2012](https://arxiv.org/abs/1202.3665)\n",
    " \n",
    "\n",
    "* Hamiltonian Monte Carlo\n",
    "* No-U-Turn Sampler\n",
    "* Hessian-Hamiltonian Monte Carlo (H2MC)\n",
    "* Gibbs Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important concepts\n",
    "* Convergence\n",
    " - Diagnostics\n",
    "   - Geweke, Raftery & Lewis, Gelman & Rubin; [See notes](https://peekxc.github.io/resources/stochastic_project.pdf)\n",
    "\n",
    "\n",
    "* Exploration Quality (EQ): An important quality measure for an MCMC algorithm is the fraction of runs which provide a representative sample from the posterior distribution for a given finite number of iterations.\n",
    "\n",
    "\n",
    "* Effective Sample Size (ESS): The ESS accounts for the in-chain autocorrelation and is an important measure for the quality of the posterior approximation for individual chains. As the ESS is overestimated if chains sample only from individual modes of the posterior distribution, we only considered chains assigned to groups which explore the posterior well. For these chains, autocorrelation for individual parameters θ i is determined using Sokal’s adaptive truncated periodogram estimator [28, 49] which is implemented in the DRAM toolbox [28]. As this is a univariate measure, we take the maximum of the autocorrelation across all θ i to determine the ESS and to thin the chain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications\n",
    "* Benchmark problems in biology [Ballnus+2017](https://bmcsystbiol.biomedcentral.com/articles/10.1186/s12918-017-0433-1)\n",
    "* oddly-shaped hypothetical distributions; see [dynesty examples](https://dynesty.readthedocs.io/en/latest/examples.html#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Software\n",
    "* packages\n",
    " - python\n",
    "   - [emcee]()\n",
    "   - nestle by Kyle Barbary\n",
    "   - dynesty by Josh Speagle\n",
    " - R\n",
    "   - [ggmcmc](https://cran.r-project.org/web/packages/ggmcmc/); [paper](https://cran.r-project.org/web/packages/ggmcmc/vignettes/v70i09.pdf)\n",
    "   \n",
    "* website\n",
    " - https://github.com/mattpitkin/theonlinemcmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
