{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# skxplore\n",
    "## A top layer package for scikit-learn\n",
    "\n",
    "This scikit-learn top layer package finds the machine learning model and hyperparameters best-suited for the dataset and properties the user has set.\n",
    "\n",
    "The find_model function has the following parameters:<br>\n",
    "`dataset` takes a pandas dataframe as its value<br>\n",
    "`train_size` is a float from the interval (0, 1); it defines the partition of the dataset for training and testing<br>\n",
    "`problem` has three selections: classification, regression, and clustering<br>\n",
    "`label` takes a column name in the pandas dataframe and treats it as the label for classification or the target value for regression<br>\n",
    "`dim_reduction` takes True or False as its value; it gives the option to apply dimensionality reduction on the dataset; False by default<br>\n",
    "`features` is the number of components to remain after dimensionality reduction; auto by default<br>\n",
    "`contains_negative` takes True or False as its value; setting its value to True uses principal component analysis, while setting its value to False uses non-negative matrix factorization; True by default<br>\n",
    "`ensembling` takes True or False as its value; setting its value to True makes use of ensemble methods from base estimators, while setting its value to False disables ensembling<br>\n",
    "`priority` has two selections: accuracy and time; selecting accuracy would enable the module to find for better hyperparameters and optimize the different algorithms in consideration; selecting time would use the default hyperparameters<br>\n",
    "\n",
    "skxplore considers the following algorithms:<br>\n",
    "1. Classification<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;`Naive Bayes algorithm`, `K-nearest neighbors algorithm`, `Support vector machine classifier`, `eXtreme gradient boosting classifier`, and `Light gradient boosting machine classifier`\n",
    "2. Regression<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;`Lasso regression`, `Ridge regression`, `Elastic net regression`, `Linear regression`, `Support vector machine regressor`, `eXtreme gradient boosting regressor`, and `Light gradient boosting machine regressor`\n",
    "3. Clustering<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;`K-means clustering`, `Spectral clustering`, `Gaussian mixture model`, `Density-based spatial clustering of applications with noise (DBSCAN) algorithm`, and `Ordering points to identify the clustering structure (OPTICS) algorithm`"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the preprocessing libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA, NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Importing the classifiers\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Importing the regressors\n",
    "from sklearn.linear_model import Lasso, Ridge, LinearRegression, ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# Importing the clustering algorithms\n",
    "from sklearn.cluster import KMeans, SpectralClustering, DBSCAN, OPTICS\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier\n",
    "from sklearn.metrics import accuracy_score, explained_variance_score, silhouette_score\n",
    "from sklearn.utils.testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "from IPython.display import HTML\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.animation as animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_model(dataset, train_size, problem, label=\"\",\n",
    "               dim_reduction=False, components=\"auto\",\n",
    "               contains_negative=True, ensembling=True, priority=\"accuracy\"):\n",
    "    print(\"Label encoding. . .\")\n",
    "    dataset = dataset.apply(LabelEncoder().fit_transform)\n",
    "    \n",
    "    # Splitting the dataset into the features and label\n",
    "    if problem != \"clustering\":\n",
    "        print(\"Identifying feature columns and label column. . .\")\n",
    "        X = dataset[dataset.columns.difference([label])] #features\n",
    "        y = dataset[label] #label\n",
    "    else:\n",
    "        print(\"Clustering problem detected, skipping label column identification. . .\")\n",
    "        X = dataset\n",
    "    \n",
    "    # Reduce dimensionality of dataset\n",
    "    if dim_reduction:\n",
    "        print(\"Performing dimensionality reduction. . .\")\n",
    "        print(\"Features' shape before reduction is\", X.shape)\n",
    "        if contains_negative: # If dataset contains negative values, use principal component analysis\n",
    "            if components == \"auto\":\n",
    "                print(\"Using default number of components for principal component analysis. . .\")\n",
    "                pca = PCA(n_components=2)\n",
    "            else:\n",
    "                print(\"Using\", components, \"components for principal component analysis. . .\")\n",
    "                pca = PCA(n_components=components)\n",
    "            X = pca.fit_transform(X)\n",
    "        else: # Otherwise, use non-negative matrix factorization\n",
    "            if components == \"auto\":\n",
    "                print(\"Using default number of components for non-negative matrix factorization. . .\")\n",
    "                nmf = NMF(n_components=2)\n",
    "            else:\n",
    "                print(\"Using\", components, \"components for principal component analysis. . .\")\n",
    "                nmf = NMF(n_components=components)\n",
    "            X = nmf.fit_transform(X)\n",
    "        print(\"Features' shape after reduction is\", X.shape)\n",
    "    \n",
    "    if problem != \"clustering\":\n",
    "        # Split X and y into training and testing datasets\n",
    "        print(\"Splitting datasets for training and testing. . .\")\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = train_size)\n",
    "        # Scale variables to standardize values\n",
    "        print(\"Standardizing values. . .\")\n",
    "        sc = StandardScaler()\n",
    "        X_train = sc.fit_transform(X_train)\n",
    "        X_test = sc.transform(X_test)\n",
    "    else:\n",
    "        # Scale variables to standardize values\n",
    "        print(\"Standardizing values. . .\")\n",
    "        sc = StandardScaler()\n",
    "        X = sc.fit_transform(X)\n",
    "\n",
    "    if priority == \"accuracy\":\n",
    "        if problem == \"classification\":\n",
    "            find_classification_model(X_train, X_test, y_train, y_test, priority=\"accuracy\", ensembling=ensembling)\n",
    "        elif problem == \"regression\":\n",
    "            find_regression_model(X_train, X_test, y_train, y_test, ensembling=ensembling)\n",
    "        elif problem == \"clustering\":\n",
    "            find_clustering_model(X)\n",
    "        \n",
    "    if priority == \"time\":\n",
    "        if problem == \"classification\":\n",
    "            find_classification_model(X_train, X_test, y_train, y_test, priority=\"time\", ensembling=ensembling)\n",
    "        elif problem == \"regression\":\n",
    "            find_regression_model(X_train, X_test, y_train, y_test, ensembling=ensembling)\n",
    "        elif problem == \"clustering\":\n",
    "            find_clustering_model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_classification_model(X_train, X_test, y_train, y_test, priority=\"accuracy\", ensembling=True):\n",
    "    models = []\n",
    "    overall_accuracies = []\n",
    "    \n",
    "    if ensembling:\n",
    "        print(\"Ensembing is enabled.\")\n",
    "        # Use XGBoost\n",
    "        if priority == \"accuracy\":\n",
    "            learning_rate_list = [0.1, 0.01, 0.001]\n",
    "            gamma_list = [0, 1, 5]\n",
    "            colsample_bytree_list = [0.3, 0.5, 0.8, 1]\n",
    "            # Creating list of cv scores\n",
    "            scores = []\n",
    "            params = []\n",
    "            \n",
    "            for lr in learning_rate_list:\n",
    "                for g in gamma_list:\n",
    "                    for cb in colsample_bytree_list:\n",
    "                        xgb = XGBClassifier(learning_rate=lr, gamma=g, colsample_bytree=cb)\n",
    "                        xgb.fit(X_train, y_train)\n",
    "                        y_pred = xgb.predict(X_test)\n",
    "                        scores.append(accuracy_score(y_test, y_pred))\n",
    "                        params.append([lr, g, cb])\n",
    "            XGB_max_scores = max(scores)\n",
    "            print(\"XGBoost classifier score is:\", XGB_max_scores, \"with the following values\\nfor learning rate, g, and number of columns used by each tree:\", params[scores.index(XGB_max_scores)])\n",
    "            lr_best = params[scores.index(XGB_max_scores)][0] \n",
    "            g_best = params[scores.index(XGB_max_scores)][1]\n",
    "            cb_best = params[scores.index(XGB_max_scores)][2]\n",
    "            models.append(XGBClassifier(learning_rate=lr_best, gamma=g_best, colsample_bytree=cb_best))\n",
    "            overall_accuracies.append(XGB_max_scores)\n",
    "            \n",
    "        if priority == \"time\":\n",
    "            xgb = XGBClassifier()\n",
    "            xgb.fit(X_train, y_train)\n",
    "            y_pred = xgb.predict(X_test)\n",
    "            print(\"XGBoost classifier score is:\", accuracy_score(y_test, y_pred), \"using\\n\", xgb)\n",
    "            models.append(XGBClassifier())\n",
    "            overall_accuracies.append(accuracy_score(y_test, y_pred))\n",
    "            \n",
    "            # Use LightGBM\n",
    "            learning_rate_list = [0.1, 0.01, 0.001]\n",
    "            objective_list = [\"binary\", \"multiclass\"]\n",
    "            colsample_bytree_list = [0.3, 0.5, 0.8, 1]\n",
    "            # Creating list of cv scores\n",
    "            scores = []\n",
    "            params = []\n",
    "            \n",
    "            for lr in learning_rate_list:\n",
    "                for o in objective_list:\n",
    "                    for cb in colsample_bytree_list:\n",
    "                        lgbm = LGBMClassifier(learning_rate=lr, objective=o, colsample_bytree=cb)\n",
    "                        lgbm.fit(X_train, y_train)\n",
    "                        y_pred = lgbm.predict(X_test)\n",
    "                        scores.append(accuracy_score(y_test, y_pred))\n",
    "                        params.append([lr, o, cb])\n",
    "            LGBM_max_scores = max(scores)\n",
    "            print(\"LightGBM classifier score is:\", LGBM_max_scores, \"with the following values\\nfor learning rate and objective:\", params[scores.index(LGBM_max_scores)])\n",
    "            lr_best = params[scores.index(LGBM_max_scores)][0] \n",
    "            o_best = params[scores.index(LGBM_max_scores)][1]\n",
    "            cb_best = params[scores.index(LGBM_max_scores)][2]\n",
    "            models.append(LGBMClassifier(learning_rate=lr_best, objective=o_best, colsample_bytree=cb_best))\n",
    "            overall_accuracies.append(LGBM_max_scores)\n",
    "            \n",
    "    else:\n",
    "        print(\"Ensembling is disabled.\")\n",
    "        \n",
    "    if priority == \"accuracy\": # If the priority is to have the highest accuracy possible\n",
    "        # Use SVM\n",
    "        # Creating lists of gamma and c for SVM\n",
    "        gamma_list = [1e-3, 1e-5, 1e-7, 1e-9]\n",
    "        c_list = [1, 10, 100, 1000]\n",
    "        kernel_list = [\"linear\", \"rbf\", \"poly\"]\n",
    "        # Creating list of cv scores\n",
    "        scores = []\n",
    "        params = []\n",
    "\n",
    "        # Perform gridsearch on SVC model\n",
    "        for c in c_list:\n",
    "            for g in gamma_list:\n",
    "                for k in kernel_list:\n",
    "                    if ensembling:\n",
    "                        svc = AdaBoostClassifier(SVC(probability=True, gamma=g, C=c, kernel=k), n_estimators=15)\n",
    "                    else:\n",
    "                        svc = SVC(gamma=g, C=c, kernel=k)\n",
    "                    svc.fit(X_train, y_train)\n",
    "                    y_pred = svc.predict(X_test)\n",
    "                    scores.append(accuracy_score(y_test, y_pred))\n",
    "                    params.append([g, c, k])\n",
    "        SVC_max_scores = max(scores)\n",
    "        print(\"Support vector classifier score is:\", SVC_max_scores, \"with the following values\\nfor g, c, and k:\", params[scores.index(SVC_max_scores)])\n",
    "        g_best = params[scores.index(SVC_max_scores)][0]\n",
    "        c_best = params[scores.index(SVC_max_scores)][1]\n",
    "        k_best = params[scores.index(SVC_max_scores)][2]\n",
    "        if ensembling:\n",
    "            models.append(AdaBoostClassifier(SVC(probability=True, gamma=g_best, C=c_best, kernel=k_best), n_estimators=15))\n",
    "        else:\n",
    "            models.append(SVC(gamma=g_best, C=c_best, kernel=k_best))\n",
    "        overall_accuracies.append(SVC_max_scores)\n",
    "\n",
    "        # Use KNN\n",
    "        # Creating list of neighbors for KNN\n",
    "        neighbor_list = [2, 3, 5, 10, 15, 20]\n",
    "        # Creating list of cv scores\n",
    "        scores = []\n",
    "        params = []\n",
    "\n",
    "        # Perform gridsearch on KNN model\n",
    "        for n in neighbor_list:\n",
    "            if len(X_train[0]) <= n:\n",
    "                if ensembling:\n",
    "                    knn = BaggingClassifier(base_estimator=KNeighborsClassifier(n_neighbors=n), n_estimators=100)\n",
    "                else:\n",
    "                    knn = KNeighborsClassifier(n_neighbors=n)\n",
    "                knn.fit(X_train, y_train)\n",
    "                y_pred = knn.predict(X_test)\n",
    "                scores.append(accuracy_score(y_test, y_pred))\n",
    "                params.append(n)\n",
    "        KNN_max_scores = max(scores)\n",
    "        print(\"K-nearest neighbor classifier score is:\", KNN_max_scores, \"with the k value =\", params[scores.index(KNN_max_scores)])\n",
    "        k_best = params[scores.index(KNN_max_scores)]\n",
    "        if ensembling:\n",
    "            models.append(BaggingClassifier(base_estimator=KNeighborsClassifier(n_neighbors=k_best), n_estimators=100))\n",
    "        else:\n",
    "            models.append(KNeighborsClassifier(n_neighbors=k_best))\n",
    "        overall_accuracies.append(KNN_max_scores)\n",
    "            \n",
    "        if ensembling:\n",
    "            nbc = AdaBoostClassifier(GaussianNB(), n_estimators=100)\n",
    "        else:\n",
    "            nbc = GaussianNB()\n",
    "        nbc.fit(X_train, y_train)\n",
    "        y_pred = nbc.predict(X_test)\n",
    "        NB_max_score = accuracy_score(y_test, y_pred)\n",
    "        print(\"Naive Bayes classifier score is:\", NB_max_score, \"using\\n\", nbc)\n",
    "        if ensembling:\n",
    "            models.append(AdaBoostClassifier(GaussianNB(), n_estimators=100))\n",
    "        else:\n",
    "            models.append(GaussianNB())\n",
    "        overall_accuracies.append(NB_max_score)\n",
    "\n",
    "    else: # If the priority is to have a trained classifier model quickly\n",
    "        # Use SVM\n",
    "        if ensembling:\n",
    "            svc = AdaBoostClassifier(SVC(probability=True), n_estimators=15)\n",
    "        else:\n",
    "            svc = SVC()\n",
    "        svc.fit(X_train, y_train)\n",
    "        y_pred = svc.predict(X_test)\n",
    "        print(\"Support vector classifier score is:\", accuracy_score(y_test, y_pred), \"using\\n\", svc)\n",
    "        if ensembling:\n",
    "            models.append(AdaBoostClassifier(SVC(probability=True), n_estimators=15))\n",
    "        else:\n",
    "            models.append(SVC())\n",
    "        overall_accuracies.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "        # Use KNN\n",
    "        if ensembling:\n",
    "            knn = BaggingClassifier(base_estimator=KNeighborsClassifier(), n_estimators=100)\n",
    "        else:\n",
    "            knn = KNeighborsClassifier()\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        print(\"K-nearest neighbor classifier score is:\", accuracy_score(y_test, y_pred), \"using\\n\", knn)\n",
    "        if ensembling:\n",
    "            models.append(BaggingClassifier(base_estimator=KNeighborsClassifier(), n_estimators=100))\n",
    "        else:\n",
    "            models.append(KNeighborsClassifier())\n",
    "        overall_accuracies.append(accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        if ensembling:\n",
    "            nbc = AdaBoostClassifier(GaussianNB(), n_estimators=100)\n",
    "        else:\n",
    "            nbc = GaussianNB()\n",
    "        nbc.fit(X_train, y_train)\n",
    "        y_pred = nbc.predict(X_test)\n",
    "        NB_max_score = accuracy_score(y_test, y_pred)\n",
    "        print(\"Naive Bayes classifier score is:\", NB_max_score, \"using\\n\", nbc)\n",
    "        if ensembling:\n",
    "            models.append(AdaBoostClassifier(GaussianNB(), n_estimators=100))\n",
    "        else:\n",
    "            models.append(GaussianNB())\n",
    "        overall_accuracies.append(NB_max_score)\n",
    "            \n",
    "    top_accuracy = max(overall_accuracies)\n",
    "    best_model = models[overall_accuracies.index(top_accuracy)]\n",
    "    print(\"\\nWith consideration of\", priority, \"as the priority, the best model found for classifying the dataset is:\\n\", best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @ignore_warnings(category=ConvergenceWarning)\n",
    "# @ignore_warnings(category=FutureWarning)\n",
    "def find_regression_model(X_train, X_test, y_train, y_test, ensembling=True):\n",
    "    models = []\n",
    "    overall_accuracies = []\n",
    "    \n",
    "    if ensembling:\n",
    "        print(\"Ensembling is enabled.\")\n",
    "        learning_rate_list = [0.1, 0.01, 0.001]\n",
    "        gamma_list = [0, 1, 5]\n",
    "        colsample_bytree_list = [0.3, 0.5, 0.8, 1]\n",
    "        # Creating list of cv scores\n",
    "        scores = []\n",
    "        params = []\n",
    "\n",
    "        for lr in learning_rate_list:\n",
    "            for g in gamma_list:\n",
    "                for cb in colsample_bytree_list:\n",
    "                    xgb = XGBRegressor(learning_rate=lr, gamma=g, colsample_bytree=cb, objective=\"reg:squarederror\")\n",
    "                    xgb.fit(X_train, y_train)\n",
    "                    y_pred = xgb.predict(X_test)\n",
    "                    scores.append(explained_variance_score(y_test, y_pred))\n",
    "                    params.append([lr, g, cb])\n",
    "        XGB_max_scores = max(scores)\n",
    "        print(\"XGBoost regressor explained variance score is:\", XGB_max_scores, \"with the following values\\nfor learning rate, g, and number of columns used by each tree:\", params[scores.index(XGB_max_scores)])\n",
    "        lr_best = params[scores.index(XGB_max_scores)][0] \n",
    "        g_best = params[scores.index(XGB_max_scores)][1]\n",
    "        cb_best = params[scores.index(XGB_max_scores)][2]\n",
    "        models.append(XGBRegressor(learning_rate=lr_best, gamma=g_best, colsample_bytree=cb_best, objective=\"reg:squarederror\"))\n",
    "        overall_accuracies.append(XGB_max_scores)\n",
    "        \n",
    "        # Use LightGBM\n",
    "        learning_rate_list = [0.1, 0.01, 0.001]\n",
    "        colsample_bytree_list = [0.3, 0.5, 0.8, 1]\n",
    "        # Creating list of cv scores\n",
    "        scores = []\n",
    "        params = []\n",
    "\n",
    "        for lr in learning_rate_list:\n",
    "            for cb in colsample_bytree_list:\n",
    "                lgbm = LGBMRegressor(learning_rate=lr, colsample_bytree=cb)\n",
    "                lgbm.fit(X_train, y_train)\n",
    "                y_pred = lgbm.predict(X_test)\n",
    "                scores.append(explained_variance_score(y_test, y_pred))\n",
    "                params.append([lr, cb])\n",
    "        LGBM_max_scores = max(scores)\n",
    "        print(\"LightGBM regressor explained variance score is:\", LGBM_max_scores, \"with the following values\\nfor learning rate and objective:\", params[scores.index(LGBM_max_scores)])\n",
    "        lr_best = params[scores.index(LGBM_max_scores)][0] \n",
    "        cb_best = params[scores.index(LGBM_max_scores)][1]\n",
    "        models.append(LGBMRegressor(learning_rate=lr_best, colsample_bytree=cb_best))\n",
    "        overall_accuracies.append(LGBM_max_scores)\n",
    "        \n",
    "    else:\n",
    "        print(\"Ensembling is disabled.\")\n",
    "        alpha_list = [100, 50, 25, 10, 5, 1, 0.75, 0.5, 0.25, 0.1, 1e-5]\n",
    "        max_iter_list = [100000, 50000, 10000, 5000, 10000]\n",
    "        \n",
    "        # Use lasso regression\n",
    "        scores = []\n",
    "        params = []\n",
    "        \n",
    "        for a in alpha_list:\n",
    "            for i in max_iter_list:\n",
    "                lasso = Lasso(alpha=a, max_iter=i)\n",
    "                lasso.fit(X_train, y_train)\n",
    "                y_pred = lasso.predict(X_test)\n",
    "                scores.append(explained_variance_score(y_test, y_pred))\n",
    "                params.append([a, i])\n",
    "        lasso_max_scores = max(scores)\n",
    "        print(\"Lasso model explained variance score is:\", lasso_max_scores, \"with the values for alpha and max iterations: \", params[scores.index(lasso_max_scores)])\n",
    "        a_best = params[scores.index(lasso_max_scores)][0]\n",
    "        i_best = params[scores.index(lasso_max_scores)][1]\n",
    "        models.append(Lasso(alpha=a_best, max_iter=i_best))\n",
    "        overall_accuracies.append(lasso_max_scores)\n",
    "        \n",
    "        # Use ridge regression\n",
    "        scores = []\n",
    "        params = []\n",
    "\n",
    "        for a in alpha_list:\n",
    "            for i in max_iter_list:\n",
    "                ridge = Ridge(alpha=a, max_iter=i)\n",
    "                ridge.fit(X_train, y_train)\n",
    "                y_pred = ridge.predict(X_test)\n",
    "                scores.append(explained_variance_score(y_test, y_pred))\n",
    "                params.append([a, i])\n",
    "        ridge_max_scores = max(scores)\n",
    "        print(\"Ridge model explained variance score is:\", ridge_max_scores, \"with the values for alpha and max iterations: \", params[scores.index(ridge_max_scores)])\n",
    "        a_best = params[scores.index(ridge_max_scores)][0]\n",
    "        i_best = params[scores.index(ridge_max_scores)][1]\n",
    "        models.append(Ridge(alpha=a_best, max_iter=i_best))\n",
    "        overall_accuracies.append(ridge_max_scores)\n",
    "        \n",
    "        # Use elastic net\n",
    "        for a in alpha_list:\n",
    "            for i in max_iter_list:\n",
    "                try:\n",
    "                    elastic = ElasticNet(alpha=a, max_iter=i, l1_ratio=0.5)\n",
    "                    elastic.fit(X_train, y_train)\n",
    "                    y_pred = elastic.predict(X_test)\n",
    "                    scores.append(explained_variance_score(y_test, y_pred))\n",
    "                    params.append([a, i])\n",
    "                except:\n",
    "                    continue\n",
    "        elastic_max_scores = max(scores)\n",
    "        a_best = params[scores.index(elastic_max_scores)][0]\n",
    "        i_best = params[scores.index(elastic_max_scores)][1]\n",
    "        l1_best = 0.5\n",
    "        models.append(ElasticNet(alpha=a_best, max_iter=i_best, l1_ratio=0.5))\n",
    "        overall_accuracies.append(elastic_max_scores)\n",
    "        print(\"Elastic net regression explained variance score is:\", elastic_max_scores, \"with the values\\nfor alpha, max iterations, and l1 ratio: \", params[scores.index(elastic_max_scores)])\n",
    "        \n",
    "        # Use linear regression\n",
    "        linear = LinearRegression()\n",
    "        linear.fit(X_train, y_train)\n",
    "        y_pred = linear.predict(X_test)\n",
    "        print(\"Linear regression explained variance score is:\", explained_variance_score(y_test, y_pred), \"using\\n\", linear)\n",
    "        models.append(LinearRegression())\n",
    "        overall_accuracies.append(explained_variance_score(y_test, y_pred))\n",
    "        \n",
    "        # Use SVM\n",
    "        # Creating lists of gamma and c for SVM\n",
    "        gamma_list = [1e-3, 1e-5, 1e-7, 1e-9]\n",
    "        c_list = [1, 10, 100, 1000]\n",
    "        kernel_list = [\"linear\", \"rbf\", \"poly\"]\n",
    "        # Creating list of cv scores\n",
    "        scores = []\n",
    "        params = []\n",
    "\n",
    "        # Perform gridsearch on SVR model\n",
    "        for c in c_list:\n",
    "            for g in gamma_list:\n",
    "                for k in kernel_list:\n",
    "                    svr = SVR(gamma=g, C=c, kernel=k)\n",
    "                    svr.fit(X_train, y_train)\n",
    "                    y_pred = svr.predict(X_test)\n",
    "                    scores.append(explained_variance_score(y_test, y_pred))\n",
    "                    params.append([g, c, k])\n",
    "        SVR_max_scores = max(scores)\n",
    "        print(\"Support vector regressor explained variance score is:\", SVR_max_scores, \"with the following values\\nfor g, c, and k:\", params[scores.index(SVR_max_scores)])\n",
    "        g_best = params[scores.index(SVR_max_scores)][0]\n",
    "        c_best = params[scores.index(SVR_max_scores)][1]\n",
    "        k_best = params[scores.index(SVR_max_scores)][2]\n",
    "        models.append(SVR(gamma=g_best, C=c_best, kernel=k_best))\n",
    "        overall_accuracies.append(SVR_max_scores)\n",
    "        \n",
    "    top_accuracy = max(overall_accuracies)\n",
    "    best_model = models[overall_accuracies.index(top_accuracy)]\n",
    "    print(\"\\nThe best model found for the regression problem on the dataset is:\\n\", best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_clustering_model(X):\n",
    "    models = []\n",
    "    overall_accuracies = []\n",
    "    overall_labels = []\n",
    "    \n",
    "    # Use k-means\n",
    "    scores = []\n",
    "    labels = []\n",
    "    n_list = list(range(2, 10))\n",
    "    for n in n_list:\n",
    "        kmeans_model = KMeans(n_clusters=n, random_state=1).fit(X)\n",
    "        labels.append(kmeans_model.labels_)\n",
    "        scores.append(silhouette_score(X, kmeans_model.labels_, metric='euclidean'))\n",
    "    kmeans_max_score = max(scores)\n",
    "    print(\"K-means clustering found an optimal number of \", n_list[scores.index(kmeans_max_score)], \"clusters with a silhouette score of\", kmeans_max_score)\n",
    "    models.append(KMeans(n_clusters=n_list[scores.index(kmeans_max_score)], random_state=1))\n",
    "    overall_accuracies.append(kmeans_max_score)\n",
    "    overall_labels.append(labels[scores.index(kmeans_max_score)])\n",
    "    \n",
    "    scores = []\n",
    "    labels = []\n",
    "    # Use spectral clustering\n",
    "    for n in n_list:\n",
    "        sc_model = SpectralClustering(n_clusters=n, random_state=1).fit(X)\n",
    "        labels.append(sc_model.labels_)\n",
    "        scores.append(silhouette_score(X, sc_model.labels_, metric='euclidean'))\n",
    "    sc_max_score = max(scores)\n",
    "    print(\"Spectral clustering found an optimal number of \", n_list[scores.index(sc_max_score)], \"clusters with a silhouette score of\", sc_max_score)\n",
    "    models.append(SpectralClustering(n_clusters=n_list[scores.index(sc_max_score)], random_state=1))\n",
    "    overall_accuracies.append(sc_max_score)\n",
    "    overall_labels.append(labels[scores.index(sc_max_score)])\n",
    "    \n",
    "    # Use Gaussian mixture model\n",
    "    scores = []\n",
    "    labels = []\n",
    "    clusters = []\n",
    "    comp_list = list(range(2, 10))\n",
    "    for comp in comp_list:\n",
    "        gmm = GaussianMixture(n_components=comp).fit(X)\n",
    "        predictions = gmm.predict(X)\n",
    "        labels.append(predictions)\n",
    "        clusters.append(len(set(predictions)))\n",
    "        try:\n",
    "            scores.append(silhouette_score(X, predictions, metric='euclidean'))\n",
    "        except:\n",
    "            continue\n",
    "    gmm_max_score = max(scores)\n",
    "    print(\"Gaussian mixture model found an optimal number of\", clusters[scores.index(gmm_max_score)], \"clusters with a silhouette score of\", gmm_max_score)\n",
    "    models.append(GaussianMixture(n_components=comp_list[scores.index(gmm_max_score)]))\n",
    "    overall_accuracies.append(gmm_max_score)\n",
    "    overall_labels.append(labels[scores.index(gmm_max_score)])\n",
    "    \n",
    "    scores = []\n",
    "    labels = []\n",
    "    clusters = []\n",
    "    eps_list = [0.1, 0.5, 1, 2, 5, 10, 25, 50, 100]\n",
    "    # Use DBSCAN\n",
    "    for eps in eps_list:\n",
    "        db_model = DBSCAN(eps=eps).fit(X)\n",
    "        labels.append(db_model.labels_)\n",
    "        clusters.append(len(set(db_model.labels_)))\n",
    "        try:\n",
    "            scores.append(silhouette_score(X, db_model.labels_, metric='euclidean'))\n",
    "        except:\n",
    "            continue\n",
    "    db_max_score = max(scores)\n",
    "    print(\"Density-based spatial clustering of applications with noise (DBSCAN) algorithm\\nfound an optimal number of\", clusters[scores.index(db_max_score)], \"clusters with a silhouette score of\", db_max_score)\n",
    "    models.append(DBSCAN(eps=eps_list[scores.index(db_max_score)]))\n",
    "    overall_accuracies.append(db_max_score)\n",
    "    overall_labels.append(labels[scores.index(db_max_score)])\n",
    "    \n",
    "    scores = []\n",
    "    labels = []\n",
    "    clusters = []\n",
    "    min_samples_list = list(range(5, 101, 5))\n",
    "    # Use OPTICS\n",
    "    for min_samples in min_samples_list:\n",
    "        o_model = OPTICS(min_samples=min_samples).fit(X)\n",
    "        labels.append(o_model.labels_)\n",
    "        clusters.append(len(set(o_model.labels_)))\n",
    "        try:\n",
    "            scores.append(silhouette_score(X, o_model.labels_, metric='euclidean'))\n",
    "        except:\n",
    "            continue\n",
    "    o_max_score = max(scores)\n",
    "    print(\"Ordering points to identify the clustering structure (OPTICS) algorithm\\nfound an optimal number of\", clusters[scores.index(o_max_score)], \"clusters with a silhouette score of\", o_max_score)\n",
    "    models.append(OPTICS(min_samples=min_samples_list[scores.index(o_max_score)]))\n",
    "    overall_accuracies.append(o_max_score)\n",
    "    overall_labels.append(labels[scores.index(o_max_score)])\n",
    "    \n",
    "    if db_max_score != max(overall_accuracies) and o_max_score != max(overall_accuracies):\n",
    "        top_accuracy = max(overall_accuracies)\n",
    "        best_model = models[overall_accuracies.index(top_accuracy)]\n",
    "        best_label = overall_labels[overall_accuracies.index(top_accuracy)]\n",
    "    elif db_max_score == o_max_score or db_max_score < o_max_score:\n",
    "        top_accuracy = o_max_score\n",
    "        best_model = models[4]\n",
    "        best_label = overall_labels[4]\n",
    "    elif db_max_score == max(overall_accuracies):\n",
    "        top_accuracy = db_max_score\n",
    "        best_model = models[3]\n",
    "        best_label = overall_labels[3]\n",
    "    else:\n",
    "        top_accuracy = max(overall_accuracies)\n",
    "        best_model = models[overall_accuracies.index(top_accuracy)]\n",
    "        best_label = overall_labels[overall_accuracies.index(top_accuracy)]\n",
    "    print(\"\\nThe best clustering algorithm found for the dataset is:\\n\", best_model)\n",
    "    \n",
    "    try:\n",
    "        pca = PCA(n_components=3)\n",
    "        X = pca.fit_transform(X)\n",
    "    except:\n",
    "        exit()\n",
    "    \n",
    "    fig = plt.figure(dpi=90)\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    x = X[:,0]\n",
    "    y = X[:,1]\n",
    "    z = X[:,2]\n",
    "\n",
    "    ax.scatter(x, y, -z, zdir='z', c=best_label, depthshade=True, s=5)\n",
    "\n",
    "    def rotate(angle):\n",
    "        ax.view_init(azim=angle)\n",
    "\n",
    "    rot_animation = animation.FuncAnimation(fig, rotate, frames=np.arange(0, 362, 2), interval=100)\n",
    "    filename = \"clustering_animation.gif\"\n",
    "    rot_animation.save(filename, writer='imagemagick', dpi=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension  ...  worst radius  worst texture  worst perimeter  \\\n",
       "0                 0.07871  ...         25.38          17.33           184.60   \n",
       "1                 0.05667  ...         24.99          23.41           158.80   \n",
       "2                 0.05999  ...         23.57          25.53           152.50   \n",
       "3                 0.09744  ...         14.91          26.50            98.87   \n",
       "4                 0.05883  ...         22.54          16.67           152.20   \n",
       "\n",
       "   worst area  worst smoothness  worst compactness  worst concavity  \\\n",
       "0      2019.0            0.1622             0.6656           0.7119   \n",
       "1      1956.0            0.1238             0.1866           0.2416   \n",
       "2      1709.0            0.1444             0.4245           0.4504   \n",
       "3       567.7            0.2098             0.8663           0.6869   \n",
       "4      1575.0            0.1374             0.2050           0.4000   \n",
       "\n",
       "   worst concave points  worst symmetry  worst fractal dimension  \n",
       "0                0.2654          0.4601                  0.11890  \n",
       "1                0.1860          0.2750                  0.08902  \n",
       "2                0.2430          0.3613                  0.08758  \n",
       "3                0.2575          0.6638                  0.17300  \n",
       "4                0.1625          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "dataset = datasets.load_breast_cancer()\n",
    "df = pd.DataFrame(dataset.data, columns=dataset.feature_names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Main parameters are the dataset, training size, and problem\n",
    "find_model(df, 1, \"clustering\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Main parameters are the dataset, training size, and problem\n",
    "find_model(df, 0.8, \"classification\", label=\"type\", dim_reduction=True,\n",
    "           components=\"auto\", contains_negative=True, ensembling=True, priority=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  \n",
       "0     15.3  396.90   4.98  \n",
       "1     17.8  396.90   9.14  \n",
       "2     17.8  392.83   4.03  \n",
       "3     18.7  394.63   2.94  \n",
       "4     18.7  396.90   5.33  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "dataset = datasets.load_boston()\n",
    "df = pd.DataFrame(dataset.data, columns=dataset.feature_names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label encoding. . .\n",
      "Identifying feature columns and label column. . .\n",
      "Splitting datasets for training and testing. . .\n",
      "Standardizing values. . .\n",
      "Ensembling is disabled.\n",
      "Lasso model explained variance score is: 0.6083777280996066 with the values for alpha and max iterations:  [0.1, 100000]\n",
      "Ridge model explained variance score is: 0.6079725499720576 with the values for alpha and max iterations:  [1e-05, 100000]\n",
      "Elastic net regression explained variance score is: 0.6079725499720576 with the values\n",
      "for alpha, max iterations, and l1 ratio:  [1e-05, 100000]\n",
      "Linear regression explained variance score is: 0.6079725506685174 using\n",
      " LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n",
      "Support vector regressor explained variance score is: 0.5862416046116055 with the following values\n",
      "for g, c, and k: [0.001, 1000, 'rbf']\n",
      "\n",
      "The best model found for the regression problem on the dataset is:\n",
      " Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=100000,\n",
      "      normalize=False, positive=False, precompute=False, random_state=None,\n",
      "      selection='cyclic', tol=0.0001, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "# Main parameters are the dataset, training size, and problem\n",
    "find_model(df, 0.8, \"regression\", label=\"TAX\", dim_reduction=False,\n",
    "           components=\"auto\", contains_negative=False, ensembling=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## auto-sklearn"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install auto-sklearn"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import autosklearn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
